# paper_agent/embeddings/embedding_model.py

from sentence_transformers import SentenceTransformer
from typing import List, Optional, Union
import numpy as np

from utils.config import config
from utils.logger import logger


class EmbeddingModel:
    """
    Manages the loading and usage of a sentence transformer model
    to generate embeddings for text.
    """

    def __init__(self):
        self.model_name = config.get("EMBEDDING_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2")
        self.model: Optional[SentenceTransformer] = None
        self._load_model()
        logger.info(f"EmbeddingModel initialized with model: {self.model_name}")

    def _load_model(self):
        """
        Loads the SentenceTransformer model.
        This will download the model if it's not already cached locally.
        """
        try:
            # Check if model is already loaded
            if self.model is None:
                logger.info(f"Loading embedding model: {self.model_name}...")
                self.model = SentenceTransformer(self.model_name)
                logger.info(f"Embedding model '{self.model_name}' loaded successfully.")
            else:
                logger.debug(f"Embedding model '{self.model_name}' already loaded.")
        except Exception as e:
            logger.error(f"Failed to load embedding model '{self.model_name}': {e}")
            self.model = None  # Ensure model is None on failure
            raise

    def get_embedding(self, text: Union[str, List[str]]) -> Optional[np.ndarray]:
        """
        Generates an embedding vector for a given text or list of texts.
        Returns a numpy array.
        """
        if self.model is None:
            logger.error("Embedding model is not loaded. Cannot generate embeddings.")
            return None
        if not text:
            logger.warning("Attempted to get embedding for empty text. Returning None.")
            return None

        try:
            # The encode method handles both single string and list of strings
            embeddings = self.model.encode(text, convert_to_numpy=True)
            return embeddings
        except Exception as e:
            logger.error(f"Error generating embedding for text (first 50 chars: '{text[:50]}...'): {e}")
            return None

    def get_embedding_dimension(self) -> Optional[int]:
        """
        Returns the dimension of the embeddings generated by the model.
        """
        if self.model is None:
            logger.warning("Embedding model not loaded, cannot determine dimension.")
            return None
        try:
            # Generate a dummy embedding to get the dimension
            dummy_embedding = self.model.encode("test", convert_to_numpy=True)
            return dummy_embedding.shape[0]
        except Exception as e:
            logger.error(f"Error determining embedding dimension: {e}")
            return None


# For testing purposes
if __name__ == "__main__":
    from utils.config import config
    from utils.logger import logger

    print("--- Testing EmbeddingModel ---")
    try:
        embedder = EmbeddingModel()

        # Test single sentence embedding
        sentence1 = "This is a test sentence for embedding."
        embedding1 = embedder.get_embedding(sentence1)
        if embedding1 is not None:
            print(f"Embedding for '{sentence1}' generated. Shape: {embedding1.shape}")
            print(f"First 10 values: {embedding1[:10]}")
        else:
            print(f"Failed to generate embedding for '{sentence1}'.")

        # Test multiple sentences embedding
        sentences = [
            "The quick brown fox jumps over the lazy dog.",
            "A fast, brown fox leaps over a sleeping canine.",
            "I love eating pizza on Fridays.",
            "The cat sat on the mat.",
        ]
        embeddings_batch = embedder.get_embedding(sentences)
        if embeddings_batch is not None:
            print(f"\nEmbeddings for batch of {len(sentences)} sentences generated. Shape: {embeddings_batch.shape}")
            print("First sentence embedding (first 10 values):", embeddings_batch[0, :10])
            print("Second sentence embedding (first 10 values):", embeddings_batch[1, :10])

            # Calculate cosine similarity between first two sentences (semantically similar)
            from sklearn.metrics.pairwise import cosine_similarity

            if embeddings_batch.shape[0] >= 2:
                similarity = cosine_similarity(embeddings_batch[0].reshape(1, -1), embeddings_batch[1].reshape(1, -1))[0][0]
                print(f"\nCosine similarity between '{sentences[0]}' and '{sentences[1]}': {similarity:.4f}")
                # Expect high similarity for semantically similar sentences

            # Calculate cosine similarity between first and third sentences (semantically dissimilar)
            if embeddings_batch.shape[0] >= 3:
                similarity = cosine_similarity(embeddings_batch[0].reshape(1, -1), embeddings_batch[2].reshape(1, -1))[0][0]
                print(f"Cosine similarity between '{sentences[0]}' and '{sentences[2]}': {similarity:.4f}")
                # Expect low similarity for semantically dissimilar sentences

        else:
            print("Failed to generate embeddings for the batch of sentences.")

        # Get embedding dimension
        dim = embedder.get_embedding_dimension()
        if dim:
            print(f"\nEmbedding dimension: {dim}")

    except Exception as e:
        logger.error(f"An error occurred during EmbeddingModel testing: {e}")
